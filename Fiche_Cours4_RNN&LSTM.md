# ğŸ§  Fiche de Cours â€“ RÃ©seaux de Neurones RÃ©currents (RNN & LSTM)
## ğŸ” 1. RÃ©seaux de Neurones RÃ©currents (RNN)
ğŸ“Œ DÃ©finition :
Contrairement aux rÃ©seaux classiques (feedforward), les RNN traitent des donnÃ©es sÃ©quentielles (texte, audio, vidÃ©o).

Le rÃ©seau mÃ©morise des Ã©tats passÃ©s pour influencer les prÃ©dictions futures.

ğŸ“¦ Architecture :
Chaque cellule RNN prend en entrÃ©e lâ€™Ã©tat cachÃ© prÃ©cÃ©dent et lâ€™entrÃ©e actuelle.

Sortie Ã  chaque Ã©tape : dÃ©pend de lâ€™entrÃ©e actuelle et de la mÃ©moire (Ã©tat cachÃ©).

Formule dâ€™actualisation :
hâ‚œ = f(hâ‚œâ‚‹â‚, xâ‚œ)


![image](https://github.com/user-attachments/assets/53415810-508e-400e-9804-7696f93cb125)


## ğŸ“ 2. Applications typiques


![image](https://github.com/user-attachments/assets/c7a6f27e-37d2-4b90-b9be-d45fb040a0d8)

Analyse de sentiments

Traduction automatique

PrÃ©diction de mots

Sous-titrage dâ€™images/vidÃ©os

## âš ï¸ 3. ProblÃ¨me du RNN classique : Vanishing Gradient
âŒ ProblÃ¨me :
Lors de l'entraÃ®nement par rÃ©tropropagation, les gradients deviennent trop petits.

RÃ©sultat : le rÃ©seau oublie les informations lointaines dans une sÃ©quence.

## ğŸ” 4. LSTM â€“ Long Short Term Memory


![image](https://github.com/user-attachments/assets/3ab28d00-9c09-4fe5-aab1-dafa2047197c)


âœ… Solution :
Une architecture de RNN Ã©quipÃ©e de portes de contrÃ´le pour gÃ©rer la mÃ©moire Ã  long terme.

Ajoute une nouvelle variable mÃ©moire : câ‚œ (cell state)


![image](https://github.com/user-attachments/assets/1ab2c992-a4c3-44a2-bafe-63f2a1047d99)


## ğŸ§± 5. Composants du LSTM
1. ğŸ§½ Forget Gate (Porte dâ€™oubli)
DÃ©cide quelles informations passÃ©es doivent Ãªtre oubliÃ©es

Utilise une sigmoÃ¯de pour produire un filtre entre 0 (oubli total) et 1 (mÃ©moire complÃ¨te)

python
Copier
Modifier
fâ‚œ = Ïƒ(Wf Â· [hâ‚œâ‚‹â‚, xâ‚œ] + bf)

![image](https://github.com/user-attachments/assets/f543f44c-5a14-404b-9bf4-86eeff76f1c2)

2. ğŸŸ© Input Gate (Porte dâ€™entrÃ©e)
SÃ©lectionne quelles nouvelles informations stocker

Met Ã  jour la mÃ©moire avec de nouvelles donnÃ©es importantes

python
Copier
Modifier
iâ‚œ = Ïƒ(Wi Â· [hâ‚œâ‚‹â‚, xâ‚œ] + bi)
cÌƒâ‚œ = tanh(Wc Â· [hâ‚œâ‚‹â‚, xâ‚œ] + bc)

![image](https://github.com/user-attachments/assets/0b24dd0f-9bcf-4eef-ab35-0d45c59d22f8)

3. ğŸ” Cell State Update
Combine les anciennes informations et les nouvelles :

python
Copier
Modifier
câ‚œ = fâ‚œ * câ‚œâ‚‹â‚ + iâ‚œ * cÌƒâ‚œ
4. ğŸ“¤ Output Gate (Porte de sortie)
DÃ©termine la sortie de la cellule et le nouvel Ã©tat cachÃ©

python
Copier
Modifier
oâ‚œ = Ïƒ(Wo Â· [hâ‚œâ‚‹â‚, xâ‚œ] + bo)
hâ‚œ = oâ‚œ * tanh(câ‚œ)


![image](https://github.com/user-attachments/assets/46e4eb50-15e4-436b-978f-48ec45a5e089)

## ğŸ§  RÃ©sumÃ© visuel â€“ Fonctionnement du LSTM
vbnet
Copier
Modifier
       EntrÃ©e xâ‚œ
           â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Forget     â”‚ â†’ filtre de mÃ©moire passÃ©e
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Input      â”‚ â†’ ajout d'infos utiles
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Cell state  â”‚ â† mÃ©moire Ã  long terme
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Output     â”‚ â†’ sortie hâ‚œ
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
