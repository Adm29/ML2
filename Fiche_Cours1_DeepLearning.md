# üß† Introduction au Deep Learning

## 1. D√©finition du Machine Learning (ML)
Le ML consiste √† d√©velopper un **mod√®le pr√©dictif** √† partir de donn√©es, en utilisant un **algorithme d‚Äôoptimisation** qui minimise l‚Äôerreur entre les pr√©dictions du mod√®le et les valeurs r√©elles.

## 2. Types de mod√®les en ML
- R√©gression lin√©aire
- Arbres de d√©cision
- SVM (Support Vector Machines)  
Chaque mod√®le utilise son propre algorithme d‚Äôoptimisation.

![image](https://github.com/user-attachments/assets/39169567-56b5-40f5-803b-fb38de95827e)


## 3. Machine Learning vs Deep Learning

| Machine Learning | Deep Learning |
|------------------|---------------|
| Mod√®les simples (r√©gression, SVM, etc.) | R√©seaux de neurones complexes |
| Repr√©sentations explicites | Repr√©sentation automatique |
| Moins gourmand en donn√©es | Requiert beaucoup de donn√©es |
| N√©cessite l‚Äôing√©nierie de caract√©ristiques | Apprend √† partir des donn√©es brutes |

## 4. Pourquoi le Deep Learning a explos√© ?
- Disponibilit√© de grandes quantit√©s de donn√©es
- Am√©lioration des capacit√©s de calcul (CPU, GPU)
- Algorithmes puissants

## 5. Neurone et perceptron

## 5.1. Motivation 
- Cerveau humain = parall√®le, adaptatif, tol√©rant aux pannes
- Le neurone artificiel imite les fonctions du neurone biologique

## 5.2. Perceptron
- Le perceptron repose sur la plasticit√© synaptique

  ![image](https://github.com/user-attachments/assets/0d0f40c6-e031-42b5-be5d-cf95db98e07b)

## 5.3 Fonctions d'activations

![image](https://github.com/user-attachments/assets/bea94736-a3a6-49dc-a248-cfaf5b30d0e3)



## 6. Histoire du Deep Learning
### D√©buts :
- 1943 : McCulloch & Pitts
- 1949 : R√®gle de Hebb

### √Çge d'or :
- 1957 : Perceptron (Rosenblatt)
- 1960 : ADALINE
- 1969 : Limites du perceptron (Minsky & Papert)

### Renaissance :
- 1982 : R√©seaux de Hopfield
- 1986 : Perceptrons multicouches (MLP)
- Depuis 2010s : CNN, LSTM, Transformer...

## 7. Structure d‚Äôun r√©seau de neurones
- Couche d‚Äôentr√©e : donn√©es brutes
- Couches cach√©es : transformations non lin√©aires
- Couche de sortie : pr√©diction

![image](https://github.com/user-attachments/assets/a91b52fa-b3d3-4562-a298-4aa823361088)


## 8. Forward Propagation
- Calcul de la sortie via `z = w¬∑x + b` et `a = activation(z)`

![image](https://github.com/user-attachments/assets/7071a315-6794-409e-b1c6-e58f0a8aa4c9)


## 9. Fonction de co√ªt
- MSE pour la r√©gression

![image](https://github.com/user-attachments/assets/b5785094-825f-467f-be6c-4b31e1c6b42a)


- Entropie crois√©e pour la classification

![image](https://github.com/user-attachments/assets/d1745c40-c334-4a17-b5be-fe90bc9c6d23)


## 10. Backpropagation

Calcul des d√©riv√©es (gradients) de la fonction de co√ªt par rapport aux poids

Application de la r√®gle de la cha√Æne pour chaque couche

Permet de mettre √† jour les poids

![image](https://github.com/user-attachments/assets/6ce41d60-10cf-46be-a3ae-836e32264220)


## 11. Descente de Gradient
Principe :
Mise √† jour des poids dans la direction oppos√©e au gradient

But : r√©duire l‚Äôerreur

Variantes :
Batch Gradient Descent : mise √† jour apr√®s tout le dataset

Stochastic Gradient Descent (SGD) : mise √† jour apr√®s chaque exemple

Mini-batch Gradient Descent : compromis entre les deux
## 12. Apprentissage par √©poque

- It√©ration sur tout le dataset
- Apprentissage par observation ou par lot

Initialisation al√©atoire des poids

Entr√©e des donn√©es

Propagation avant (calcul de la sortie)

√âvaluation de l‚Äôerreur

R√©tropropagation (calcul des gradients)

Mise √† jour des poids

R√©p√©ter pour plusieurs √©poques

## 13. Optimisation de la config ANN
üîß Param√®tres essentiels :

Taux d‚Äôapprentissage Œ± : d√©termine la vitesse de convergence

Dropout : d√©sactive certains neurones pendant l‚Äôapprentissage ‚Üí √©vite le surapprentissage

Weight Decay : ajoute une p√©nalit√© sur les grands poids pour une meilleure g√©n√©ralisation

## 14. Deep Learning Moderne
‚úÖ Fonctions d‚Äôactivation :
ReLU, Sigmoid, Tanh, Softmax, Leaky ReLU‚Ä¶

‚úÖ R√©seaux de neurones convolutifs (CNN)
Utilis√©s en vision par ordinateur

Extraient automatiquement les caract√©ristiques spatiales

‚úÖ R√©seaux r√©currents (RNN / LSTM)
Traitent les donn√©es s√©quentielles (texte, s√©ries temporelles)

‚úÖ Transformer
Utilis√© en traitement du langage naturel (ex : GPT, BERT)

Repose sur le m√©canisme d‚Äôattention

![image](https://github.com/user-attachments/assets/4c8ec325-8626-4112-b97e-6bf668c8e5fc)

